{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Unlike English, Chinese sentences consist of characters without blanks, for example, \"I came to this restaurant this afternoon\"(我今天下午来到了这个饭店。) and sometimes one character can represent a term, soemtimes two or more characters represent a term together. Humans can identify those terms according to their knowledge and the context, but not for computers. In order to do Chinese document classification, we need to segment Chinese texts into terms of characters first, then extract features as in English document classification, and finally classify the texts using machine learning or deep learning methods."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Read Data\n",
    "\n",
    "We need to read the original Chinese texts and segment them. It is necessary to open the files with encoding mode that matches Chinese. After segmentation, we save those terms in new files. The segmentation will be done by Jieba package which makes use of Conditional Random Fields algorithms."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import os\n",
    "import sys\n",
    "import glob\n",
    "import jieba\n",
    "import codecs\n",
    "class textReader(object):\n",
    "    '''Read text files and split them into Chinese terms'''\n",
    "    def __init__(self, corpus_path):\n",
    "        self.corpus_path = corpus_path\n",
    "\n",
    "    def __fetchFilenames(self):\n",
    "        # Get child directories, each child directory means a class\n",
    "        categories = os.listdir(self.corpus_path)\n",
    "        #Fetch file names within each folder\n",
    "        file_label_dict = {}\n",
    "        #Fetch file names for each category\n",
    "        try:\n",
    "            for d in categories:\n",
    "                path = self.corpus_path + '\\\\' + d\n",
    "                file_names = glob.glob(path+'\\\\'+'*.txt')\n",
    "                file_label_dict[d] = file_names\n",
    "        except:\n",
    "            print('Files Reading Error!')\n",
    "        return  file_label_dict\n",
    "\n",
    "    def fetchAllFilePaths(self):\n",
    "        '''Get all the file names and their labels'''\n",
    "        file_label_dict = self.__fetchFilenames()\n",
    "        filepaths = []\n",
    "        labels = []\n",
    "        for k,v in file_label_dict.items():\n",
    "            for f in v:\n",
    "                labels.append(k)\n",
    "                filepaths.append(f)\n",
    "        return filepaths, labels\n",
    "    \n",
    "    def __loadChineseText(self, path):\n",
    "        #Handle Chinese Characters\n",
    "        with codecs.open(f, encoding='gbk', errors='ignore') as fi:\n",
    "            #Load the text and remove blanks and endings\n",
    "            text = fi.read()\n",
    "            text = text.replace(\"\\r\\n\", \"\") \n",
    "            text = text.replace(\" \", \"\")\n",
    "        return text\n",
    "    \n",
    "    def generateTextSegments(self, savePath):\n",
    "        '''Create Segmented texts'''\n",
    "        filepaths, labels = self.fetchAllFilePaths()\n",
    "        if not os.path.exists(savePath):\n",
    "            os.makedirs(savePath) \n",
    "            print('Saving Folder Created')\n",
    "        #Traverse each text\n",
    "        for f in filepaths:\n",
    "            try:\n",
    "                text = self.__loadChineseText(f)\n",
    "                #Cut the text into words\n",
    "                segs = jieba.cut(text)\n",
    "                #Get the name of the file\n",
    "                basename = os.path.basename(f)\n",
    "                content = \" \".join(list(segs))\n",
    "                savefile = savePath + '\\\\' + basename\n",
    "                #Save the content in a new file\n",
    "                with open(savefile, \"w\") as fp:  \n",
    "                    fp.write(content)  \n",
    "            except:\n",
    "                print('File:', f, 'Loading Errors!')\n",
    "        print('Segmentation FInished!')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Building prefix dict from the default dictionary ...\n",
      "Loading model from cache C:\\Users\\richard\\AppData\\Local\\Temp\\jieba.cache\n",
      "Loading model cost 1.268 seconds.\n",
      "Prefix dict has been built succesfully.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Segmentation FInished!\n"
     ]
    }
   ],
   "source": [
    "path_train = 'data\\\\train'\n",
    "tr = textReader(path_train)\n",
    "filenames, labels = tr.fetchAllFilePaths()\n",
    "tr.generateTextSegments('Segmented\\\\train')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Segmentation FInished!\n"
     ]
    }
   ],
   "source": [
    "path_test = 'data\\\\test'\n",
    "tr = textReader(path_test)\n",
    "filenames, labels = tr.fetchAllFilePaths()\n",
    "tr.generateTextSegments('Segmented\\\\test')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we have segmented those Chinese texts into series of terms separated by blanks."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Text Transformation\n",
    "\n",
    "In this part, we are going to transform each term into an ID, and transform a text into a sequence of IDs."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "import re\n",
    "from sklearn.preprocessing import OneHotEncoder\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "class textTransform:\n",
    "    '''Transfor a text into a sequence of term IDs'''\n",
    "    def __init__(self, path_train, path_test, max_doc_len=300):\n",
    "        self.path_train = path_train\n",
    "        self.path_test = path_test\n",
    "        self.max_doc_len = max_doc_len\n",
    "    \n",
    "    def __loadChineseText(self, path):\n",
    "        #Handle Chinese Characters\n",
    "        with open(path) as fi:\n",
    "            #Load the text and remove blanks and endings\n",
    "            text = fi.read()\n",
    "            #text = text.replace(\"\\r\\n\", \"\") \n",
    "            #text = text.replace(\" \", \"\")\n",
    "        return text\n",
    "    \n",
    "    def __extractLabel(self, path):\n",
    "        #Get the file name\n",
    "        filename = os.path.basename(path)\n",
    "        #Remove the ending\n",
    "        filename = filename.strip('.txt')\n",
    "        #Remove digits\n",
    "        filename = re.sub('[0-9]', '', filename)\n",
    "        return filename\n",
    "        \n",
    "    def __readTextLabel(self):\n",
    "        train_file_names = glob.glob(self.path_train+'\\\\'+'*.txt')\n",
    "        test_file_names = glob.glob(self.path_test+'\\\\'+'*.txt')\n",
    "        train_files, test_files = [], []\n",
    "        train_labels, test_labels = [], []\n",
    "        for f in train_file_names:\n",
    "            try:\n",
    "                text = self.__loadChineseText(f)\n",
    "                train_files.append(text)\n",
    "                #Set the folder name as category\n",
    "                label = self.__extractLabel(f)\n",
    "                train_labels.append(label)\n",
    "            except Exception as e:\n",
    "                print('Error in ', f)\n",
    "                print(e)\n",
    "        for f in test_file_names:\n",
    "            try:\n",
    "                text = self.__loadChineseText(f)\n",
    "                test_files.append(text)\n",
    "                #Set the folder name as category\n",
    "                label = self.__extractLabel(f)\n",
    "                test_labels.append(label)\n",
    "            except:\n",
    "                print('Error in ', f)\n",
    "        return train_files, test_files, train_labels, test_labels\n",
    "    \n",
    "    def loadTextLabel(self):\n",
    "        #Get the texts and labels\n",
    "        train_files, test_files, train_labels, test_labels = self.__readTextLabel()\n",
    "        #Encode the labels\n",
    "        le = LabelEncoder()\n",
    "        train_labels = le.fit_transform(train_labels)\n",
    "        test_labels = le.transform(test_labels)\n",
    "        #Save the original labels\n",
    "        labels = le.classes_\n",
    "        return train_files, test_files, train_labels, test_labels, labels\n",
    "        \n",
    "        \n",
    "    def text2vec(self):\n",
    "        train_files, test_files, train_labels, test_labels, _ = self.loadTextLabel()\n",
    "        #Transform texts\n",
    "        vocab_processor = tf.contrib.learn.preprocessing.VocabularyProcessor(self.max_doc_len)\n",
    "        x_transform_train = vocab_processor.fit_transform(train_files)\n",
    "        x_transform_test = vocab_processor.transform(test_files)\n",
    "        x_train = np.array(list(x_transform_train))\n",
    "        x_test = np.array(list(x_transform_test))\n",
    "        #Encode the label as one-hot code\n",
    "        y_train = train_labels\n",
    "        y_test = test_labels\n",
    "        return x_train, x_test, y_train, y_test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "path_train_processed = '..\\\\data\\\\Segmented\\\\train'\n",
    "path_test_processed = '..\\\\data\\\\Segmented\\\\test'\n",
    "tt = textTransform(path_train=path_train_processed, path_test=path_test_processed)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "x_train, x_test, y_train, y_test = tt.text2vec()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "train_files, test_files, train_labels, test_labels, labels = tt.loadTextLabel()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Extract features from text\n",
    "\n",
    "Here we are going to use TfIdf method to extract features from texts, actually it is a bag-of-word model. First we build a vocabulary, then calculate TfIdf value for each word within each text. And a text can be represented as a vector of TfIdf values."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import re\n",
    "import string\n",
    "from collections import Counter\n",
    "\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "class textHelpers:\n",
    "    '''\n",
    "    Clean texts and turn them into series of numbers\n",
    "    Args:\n",
    "    train_data\n",
    "    test_data\n",
    "    '''\n",
    "    def __init__(self, train_data, test_data):\n",
    "        self._train_data = train_data\n",
    "        self._test_data = test_data\n",
    "        self._preprocess()\n",
    "    \n",
    "    \n",
    "    def _preProcessor(self, s):\n",
    "        #remove punctuation\n",
    "        s = re.sub('[。，！”“：；？（）【】√⊥×,-、( )［ ］]', ' ', s)\n",
    "        s = re.sub('[+string.punctuation+]', ' ', s)\n",
    "        #remove digits\n",
    "        s = re.sub('['+string.digits+']', ' ', s)\n",
    "        #remove foreign characters\n",
    "        s = re.sub('[a-zA-Z]', ' ', s)\n",
    "        #remove line ends\n",
    "        s = re.sub('\\n', ' ', s)\n",
    "        #s = re.sub('', ' ', s)\n",
    "        #turn to lower case\n",
    "        s = s.lower()\n",
    "        s = re.sub('[ ]+',' ', s)\n",
    "        s = s.rstrip()\n",
    "        return s\n",
    "    \n",
    "    def _preprocess(self):\n",
    "        '''Remove punctuations'''\n",
    "        train_text = self._train_data\n",
    "        test_text = self._test_data\n",
    "        self._train_data = [self._preProcessor(item) for item in train_text]\n",
    "        self._test_data = [self._preProcessor(item) for item in test_text]\n",
    "        \n",
    "    def tfidf_vectorizer(self):\n",
    "        ''''Vectorize texts'''\n",
    "        tfidfVectorizer = TfidfVectorizer(ngram_range=(1, 1), max_features=10000,\n",
    "                                         max_df=1000)\n",
    "        X_train_tfidf = tfidfVectorizer.fit_transform(self._train_data)\n",
    "        X_test_tfidf = tfidfVectorizer.transform(self._test_data)\n",
    "        vocab_index_dict = tfidfVectorizer.vocabulary_\n",
    "        return X_train_tfidf, X_test_tfidf, vocab_index_dict\n",
    "    \n",
    "    def tfidf_weight(self):\n",
    "        '''Calculate TfIdf weights for each word within each news'''\n",
    "        train_text_words, test_text_words = self._text2words()\n",
    "        X_train_tfidf, X_test_tfidf, vocab_index_dict = self.tfidf_vectorizer()\n",
    "        train_weights = []\n",
    "        test_weights = []\n",
    "        #Generate dicts for words and corresponding tfidf weights\n",
    "        for i, text in enumerate(train_text_words):\n",
    "            word_weight = []\n",
    "            for word in text:\n",
    "                try:\n",
    "                    word_index = vocab_index_dict.get(word)\n",
    "                    w = X_train_tfidf[i, word_index]\n",
    "                    word_weight.append(w)\n",
    "                except:\n",
    "                    word_weight.append(0)\n",
    "            train_weights.append(word_weight)\n",
    "        for i, text in enumerate(test_text_words):\n",
    "            word_weight = []\n",
    "            for word in news:\n",
    "                try:\n",
    "                    word_index = vocab_index_dict.get(word)\n",
    "                    w = X_test_tfidf[i, word_index]\n",
    "                    word_weight.append(w)\n",
    "                except:\n",
    "                    word_weight.append(0)\n",
    "            test_weights.append(word_weight)      \n",
    "        return train_weights, test_weights\n",
    "    \n",
    "    def _text2words(self):\n",
    "        #Split each news into words\n",
    "        train_text_words = []\n",
    "        test_text_words = []\n",
    "        for text in self._train_data:\n",
    "           #Collect words for each news\n",
    "           train_text_words.append(text.split())\n",
    "        for text in self._test_data:\n",
    "            test_text_words.append(text.split())\n",
    "        return train_text_words, test_text_words\n",
    "    \n",
    "    def buildVocab(self):\n",
    "        words = []\n",
    "        for text in self._train_data:\n",
    "           #Collect all the chars\n",
    "           words.extend(text.split())\n",
    "        #Calculate frequencies of each character\n",
    "        word_freq = Counter(words)\n",
    "        #Filter out those low frequency characters\n",
    "        vocab = [u for u,v in word_freq.items() if v>3]\n",
    "        if 'UNK' not in vocab:\n",
    "            vocab.append('UNK')\n",
    "        #Map each char into an ID\n",
    "        word_id_map = dict(zip(vocab, range(len(vocab))))\n",
    "        #Map each ID into a word\n",
    "        id_word_map = dict(zip(word_id_map.values(), word_id_map.keys()))\n",
    "        return vocab, word_id_map, id_word_map\n",
    "    \n",
    "    def text2vecs(self):\n",
    "        #Map each word into an ID\n",
    "        train_text_words, test_text_words = self._text2words()\n",
    "        vocab, word_id_map, id_word_mapp = self.buildVocab()\n",
    "        def word2id(c):\n",
    "            try:\n",
    "               ID = word_id_map[c]\n",
    "            except:#Trun those less frequent words into UNK\n",
    "               ID = word_id_map['UNK']\n",
    "            return ID\n",
    "        #Turn each news into a list of word Ids\n",
    "        words_vecs = lambda words: [word2id(w) for w in words]\n",
    "        train_text_vecs = [words_vecs(words) for words in train_text_words]\n",
    "        test_text_vecs = [words_vecs(words) for words in test_text_words]\n",
    "        return train_text_vecs, test_text_vecs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "th = textHelpers(train_files, test_files)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "X_train_tfidf, X_test_tfidf, vocab_index_dict = th.tfidf_vectorizer()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Classify the texts\n",
    "\n",
    "We can use some classical and simple models initially, then we can turn to some complex models and tune parameters to optimize the performance of classification"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from sklearn.naive_bayes import MultinomialNB\n",
    "from sklearn.metrics import f1_score\n",
    "nb = MultinomialNB(0.001)\n",
    "nb.fit(X_train_tfidf, train_labels)\n",
    "preds = nb.predict(X_test_tfidf)\n",
    "micro_f1 = f1_score(test_labels, preds, average='micro')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Multinomial Naive Bayesian Accuracy:0.873\n"
     ]
    }
   ],
   "source": [
    "print('Multinomial Naive Bayesian Accuracy:{:.3f}'.format(micro_f1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from sklearn.ensemble import RandomForestClassifier\n",
    "rf =  RandomForestClassifier(n_estimators=100, n_jobs=4)\n",
    "rf.fit(X_train_tfidf, train_labels)\n",
    "preds = rf.predict(X_test_tfidf)\n",
    "micro_f1 = f1_score(test_labels, preds, average='micro')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Random Forest Accuracy:0.883\n"
     ]
    }
   ],
   "source": [
    "print('Random Forest Accuracy:{:.3f}'.format(micro_f1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from sklearn.neural_network import MLPClassifier\n",
    "mlp = MLPClassifier(hidden_layer_sizes=(100,), max_iter=300, alpha=1e-4,\n",
    "                    solver='sgd', verbose=10, tol=1e-5, random_state=1,\n",
    "                    learning_rate_init=.17)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration 1, loss = 2.32788109\n",
      "Iteration 2, loss = 1.38315838\n",
      "Iteration 3, loss = 0.77474985\n",
      "Iteration 4, loss = 0.58069145\n",
      "Iteration 5, loss = 0.48653017\n",
      "Iteration 6, loss = 0.41358977\n",
      "Iteration 7, loss = 0.37262386\n",
      "Iteration 8, loss = 0.32696181\n",
      "Iteration 9, loss = 0.29636267\n",
      "Iteration 10, loss = 0.25800192\n",
      "Iteration 11, loss = 0.23235325\n",
      "Iteration 12, loss = 0.21063823\n",
      "Iteration 13, loss = 0.19426976\n",
      "Iteration 14, loss = 0.18600798\n",
      "Iteration 15, loss = 0.15733720\n",
      "Iteration 16, loss = 0.14324843\n",
      "Iteration 17, loss = 0.13656902\n",
      "Iteration 18, loss = 0.11369000\n",
      "Iteration 19, loss = 0.10470720\n",
      "Iteration 20, loss = 0.09336378\n",
      "Iteration 21, loss = 0.08530211\n",
      "Iteration 22, loss = 0.07750077\n",
      "Iteration 23, loss = 0.07104272\n",
      "Iteration 24, loss = 0.06459405\n",
      "Iteration 25, loss = 0.05928299\n",
      "Iteration 26, loss = 0.05535413\n",
      "Iteration 27, loss = 0.05212712\n",
      "Iteration 28, loss = 0.04909783\n",
      "Iteration 29, loss = 0.04278151\n",
      "Iteration 30, loss = 0.03941097\n",
      "Iteration 31, loss = 0.03855078\n",
      "Iteration 32, loss = 0.03971049\n",
      "Iteration 33, loss = 0.03391801\n",
      "Iteration 34, loss = 0.03009457\n",
      "Iteration 35, loss = 0.02802974\n",
      "Iteration 36, loss = 0.02669361\n",
      "Iteration 37, loss = 0.02492060\n",
      "Iteration 38, loss = 0.02397092\n",
      "Iteration 39, loss = 0.02704627\n",
      "Iteration 40, loss = 0.02114485\n",
      "Iteration 41, loss = 0.02006063\n",
      "Iteration 42, loss = 0.01910999\n",
      "Iteration 43, loss = 0.02115240\n",
      "Iteration 44, loss = 0.01749502\n",
      "Iteration 45, loss = 0.01679990\n",
      "Iteration 46, loss = 0.01848925\n",
      "Iteration 47, loss = 0.01554343\n",
      "Iteration 48, loss = 0.01476601\n",
      "Iteration 49, loss = 0.01416174\n",
      "Iteration 50, loss = 0.02164884\n",
      "Iteration 51, loss = 0.01461799\n",
      "Iteration 52, loss = 0.01304390\n",
      "Iteration 53, loss = 0.01232931\n",
      "Iteration 54, loss = 0.01181598\n",
      "Iteration 55, loss = 0.01154271\n",
      "Iteration 56, loss = 0.01109707\n",
      "Iteration 57, loss = 0.01094157\n",
      "Iteration 58, loss = 0.01057978\n",
      "Iteration 59, loss = 0.01028560\n",
      "Iteration 60, loss = 0.01010907\n",
      "Iteration 61, loss = 0.00986225\n",
      "Iteration 62, loss = 0.00988263\n",
      "Iteration 63, loss = 0.00935288\n",
      "Iteration 64, loss = 0.00908200\n",
      "Iteration 65, loss = 0.00895003\n",
      "Iteration 66, loss = 0.00869057\n",
      "Iteration 67, loss = 0.00857266\n",
      "Iteration 68, loss = 0.00851882\n",
      "Iteration 69, loss = 0.00815871\n",
      "Iteration 70, loss = 0.00805422\n",
      "Iteration 71, loss = 0.00782960\n",
      "Iteration 72, loss = 0.00795313\n",
      "Iteration 73, loss = 0.00759811\n",
      "Iteration 74, loss = 0.00759228\n",
      "Iteration 75, loss = 0.00733417\n",
      "Iteration 76, loss = 0.00722319\n",
      "Iteration 77, loss = 0.00709065\n",
      "Iteration 78, loss = 0.00698451\n",
      "Iteration 79, loss = 0.00680932\n",
      "Iteration 80, loss = 0.00670855\n",
      "Iteration 81, loss = 0.00672267\n",
      "Iteration 82, loss = 0.00661505\n",
      "Iteration 83, loss = 0.00651725\n",
      "Iteration 84, loss = 0.00631719\n",
      "Iteration 85, loss = 0.00631565\n",
      "Iteration 86, loss = 0.00630176\n",
      "Iteration 87, loss = 0.00607802\n",
      "Iteration 88, loss = 0.00613811\n",
      "Iteration 89, loss = 0.00613004\n",
      "Iteration 90, loss = 0.00614956\n",
      "Training loss did not improve more than tol=0.000010 for two consecutive epochs. Stopping.\n"
     ]
    }
   ],
   "source": [
    "mlp = mlp.fit(X_train_tfidf, train_labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Multi Linear Perceptron Accuracy:0.908\n"
     ]
    }
   ],
   "source": [
    "preds = mlp.predict(X_test_tfidf)\n",
    "micro_f1 = f1_score(test_labels, preds, average='micro')\n",
    "print('Multi Linear Perceptron Accuracy:{:.3f}'.format(micro_f1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python [default]",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
