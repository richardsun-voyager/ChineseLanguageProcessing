{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Read Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import os\n",
    "import sys\n",
    "import glob\n",
    "import jieba\n",
    "import codecs\n",
    "class textReader(object):\n",
    "    '''Read text files and split them into Chinese terms'''\n",
    "    def __init__(self, corpus_path):\n",
    "        self.corpus_path = corpus_path\n",
    "\n",
    "    def __fetchFilenames(self):\n",
    "        # Get child directories, each child directory means a class\n",
    "        categories = os.listdir(self.corpus_path)\n",
    "        #Fetch file names within each folder\n",
    "        file_label_dict = {}\n",
    "        #Fetch file names for each category\n",
    "        try:\n",
    "            for d in categories:\n",
    "                path = self.corpus_path + '\\\\' + d\n",
    "                file_names = glob.glob(path+'\\\\'+'*.txt')\n",
    "                file_label_dict[d] = file_names\n",
    "        except:\n",
    "            print('Files Reading Error!')\n",
    "        return  file_label_dict\n",
    "\n",
    "    def fetchAllFilePaths(self):\n",
    "        '''Get all the file names and their labels'''\n",
    "        file_label_dict = self.__fetchFilenames()\n",
    "        filepaths = []\n",
    "        labels = []\n",
    "        for k,v in file_label_dict.items():\n",
    "            for f in v:\n",
    "                labels.append(k)\n",
    "                filepaths.append(f)\n",
    "        return filepaths, labels\n",
    "    \n",
    "    def __loadChineseText(self, path):\n",
    "        #Handle Chinese Characters\n",
    "        with codecs.open(f, encoding='gbk', errors='ignore') as fi:\n",
    "            #Load the text and remove blanks and endings\n",
    "            text = fi.read()\n",
    "            text = text.replace(\"\\r\\n\", \"\") \n",
    "            text = text.replace(\" \", \"\")\n",
    "        return text\n",
    "    \n",
    "    def generateTextSegments(self, savePath):\n",
    "        '''Create Segmented texts'''\n",
    "        filepaths, labels = self.fetchAllFilePaths()\n",
    "        if not os.path.exists(savePath):\n",
    "            os.makedirs(savePath) \n",
    "            print('Saving Folder Created')\n",
    "        #Traverse each text\n",
    "        for f in filepaths:\n",
    "            try:\n",
    "                text = self.__loadChineseText(f)\n",
    "                #Cut the text into words\n",
    "                segs = jieba.cut(text)\n",
    "                #Get the name of the file\n",
    "                basename = os.path.basename(f)\n",
    "                content = \" \".join(list(segs))\n",
    "                savefile = savePath + '\\\\' + basename\n",
    "                #Save the content in a new file\n",
    "                with open(savefile, \"w\") as fp:  \n",
    "                    fp.write(content)  \n",
    "            except:\n",
    "                print('File:', f, 'Loading Errors!')\n",
    "        print('Segmentation FInished!')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Building prefix dict from the default dictionary ...\n",
      "Loading model from cache C:\\Users\\richard\\AppData\\Local\\Temp\\jieba.cache\n",
      "Loading model cost 1.268 seconds.\n",
      "Prefix dict has been built succesfully.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Segmentation FInished!\n"
     ]
    }
   ],
   "source": [
    "path_train = 'data\\\\train'\n",
    "tr = textReader(path_train)\n",
    "filenames, labels = tr.fetchAllFilePaths()\n",
    "tr.generateTextSegments('Segmented\\\\train')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Segmentation FInished!\n"
     ]
    }
   ],
   "source": [
    "path_test = 'data\\\\test'\n",
    "tr = textReader(path_test)\n",
    "filenames, labels = tr.fetchAllFilePaths()\n",
    "tr.generateTextSegments('Segmented\\\\test')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Text Transformation\n",
    "\n",
    "In this part, we are going to transform each term into an ID, and transform a text into a sequence of IDs."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "import re\n",
    "from sklearn.preprocessing import OneHotEncoder\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "class textTransform:\n",
    "    '''Transfor a text into a sequence of term IDs'''\n",
    "    def __init__(self, path_train, path_test, max_doc_len=300):\n",
    "        self.path_train = path_train\n",
    "        self.path_test = path_test\n",
    "        self.max_doc_len = max_doc_len\n",
    "    \n",
    "    def __loadChineseText(self, path):\n",
    "        #Handle Chinese Characters\n",
    "        with open(path) as fi:\n",
    "            #Load the text and remove blanks and endings\n",
    "            text = fi.read()\n",
    "            #text = text.replace(\"\\r\\n\", \"\") \n",
    "            #text = text.replace(\" \", \"\")\n",
    "        return text\n",
    "    \n",
    "    def __extractLabel(self, path):\n",
    "        #Get the file name\n",
    "        filename = os.path.basename(path)\n",
    "        #Remove the ending\n",
    "        filename = filename.strip('.txt')\n",
    "        #Remove digits\n",
    "        filename = re.sub('[0-9]', '', filename)\n",
    "        return filename\n",
    "        \n",
    "    def __readTextLabel(self):\n",
    "        train_file_names = glob.glob(self.path_train+'\\\\'+'*.txt')\n",
    "        test_file_names = glob.glob(self.path_test+'\\\\'+'*.txt')\n",
    "        train_files, test_files = [], []\n",
    "        train_labels, test_labels = [], []\n",
    "        for f in train_file_names:\n",
    "            try:\n",
    "                text = self.__loadChineseText(f)\n",
    "                train_files.append(text)\n",
    "                #Set the folder name as category\n",
    "                label = self.__extractLabel(f)\n",
    "                train_labels.append(label)\n",
    "            except Exception as e:\n",
    "                print('Error in ', f)\n",
    "                print(e)\n",
    "        for f in test_file_names:\n",
    "            try:\n",
    "                text = self.__loadChineseText(f)\n",
    "                test_files.append(text)\n",
    "                #Set the folder name as category\n",
    "                label = self.__extractLabel(f)\n",
    "                test_labels.append(label)\n",
    "            except:\n",
    "                print('Error in ', f)\n",
    "        return train_files, test_files, train_labels, test_labels\n",
    "    \n",
    "    def loadTextLabel(self):\n",
    "        #Get the texts and labels\n",
    "        train_files, test_files, train_labels, test_labels = self.__readTextLabel()\n",
    "        #Encode the labels\n",
    "        le = LabelEncoder()\n",
    "        train_labels = le.fit_transform(train_labels)\n",
    "        test_labels = le.transform(test_labels)\n",
    "        #Save the original labels\n",
    "        labels = l.classes_\n",
    "        return train_files, test_files, train_labels, test_labels, labels\n",
    "        \n",
    "        \n",
    "    def text2vec(self):\n",
    "        train_files, test_files, train_labels, test_labels, _ = self.loadTextLabel()\n",
    "        #Transform texts\n",
    "        vocab_processor = tf.contrib.learn.preprocessing.VocabularyProcessor(self.max_doc_len)\n",
    "        x_transform_train = vocab_processor.fit_transform(train_files)\n",
    "        x_transform_test = vocab_processor.transform(test_files)\n",
    "        x_train = np.array(list(x_transform_train))\n",
    "        x_test = np.array(list(x_transform_test))\n",
    "        #Encode the label as one-hot code\n",
    "        y_train = train_labels\n",
    "        y_test = test_labels\n",
    "        return x_train, x_test, y_train, y_test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "path_train_processed = 'Segmented\\\\train'\n",
    "path_test_processed = 'Segmented\\\\test'\n",
    "tt = textTransform(path_train=path_train_processed, path_test=path_test_processed)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "x_train, x_test, y_train, y_test = tt.text2vec()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "train_files, test_files, train_labels, test_labels, labels = tt.loadTextLabel()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Extract features from text\n",
    "\n",
    "Here we are going to use TfIdf method to extract features from texts, actually it is a bag-of-word model. First we build a vocabulary, then calculate TfIdf value for each word within each text. And a text can be represented as a vector of TfIdf values."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 148,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import re\n",
    "import string\n",
    "from collections import Counter\n",
    "\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "class textHelpers:\n",
    "    '''\n",
    "    Clean texts and turn them into series of numbers\n",
    "    Args:\n",
    "    train_data\n",
    "    test_data\n",
    "    '''\n",
    "    def __init__(self, train_data, test_data):\n",
    "        self._train_data = train_data\n",
    "        self._test_data = test_data\n",
    "        self._preprocess()\n",
    "    \n",
    "    \n",
    "    def _preProcessor(self, s):\n",
    "        #remove punctuation\n",
    "        s = re.sub('[。，！”“：；？（）【】√⊥×,-、( )［ ］]', ' ', s)\n",
    "        s = re.sub('[+string.punctuation+]', ' ', s)\n",
    "        #remove digits\n",
    "        s = re.sub('['+string.digits+']', ' ', s)\n",
    "        #remove foreign characters\n",
    "        s = re.sub('[a-zA-Z]', ' ', s)\n",
    "        #remove line ends\n",
    "        s = re.sub('\\n', ' ', s)\n",
    "        #s = re.sub('', ' ', s)\n",
    "        #turn to lower case\n",
    "        s = s.lower()\n",
    "        s = re.sub('[ ]+',' ', s)\n",
    "        s = s.rstrip()\n",
    "        return s\n",
    "    \n",
    "    def _preprocess(self):\n",
    "        '''Remove punctuations'''\n",
    "        train_text = self._train_data\n",
    "        test_text = self._test_data\n",
    "        self._train_data = [self._preProcessor(item) for item in train_text]\n",
    "        self._test_data = [self._preProcessor(item) for item in test_text]\n",
    "        \n",
    "    def tfidf_vectorizer(self):\n",
    "        ''''Vectorize texts'''\n",
    "        tfidfVectorizer = TfidfVectorizer(ngram_range=(1, 1), max_features=10000,\n",
    "                                         max_df=1000)\n",
    "        X_train_tfidf = tfidfVectorizer.fit_transform(self._train_data)\n",
    "        X_test_tfidf = tfidfVectorizer.transform(self._test_data)\n",
    "        vocab_index_dict = tfidfVectorizer.vocabulary_\n",
    "        return X_train_tfidf, X_test_tfidf, vocab_index_dict\n",
    "    \n",
    "    def tfidf_weight(self):\n",
    "        '''Calculate TfIdf weights for each word within each news'''\n",
    "        train_text_words, test_text_words = self._text2words()\n",
    "        X_train_tfidf, X_test_tfidf, vocab_index_dict = self.tfidf_vectorizer()\n",
    "        train_weights = []\n",
    "        test_weights = []\n",
    "        #Generate dicts for words and corresponding tfidf weights\n",
    "        for i, text in enumerate(train_text_words):\n",
    "            word_weight = []\n",
    "            for word in text:\n",
    "                try:\n",
    "                    word_index = vocab_index_dict.get(word)\n",
    "                    w = X_train_tfidf[i, word_index]\n",
    "                    word_weight.append(w)\n",
    "                except:\n",
    "                    word_weight.append(0)\n",
    "            train_weights.append(word_weight)\n",
    "        for i, text in enumerate(test_text_words):\n",
    "            word_weight = []\n",
    "            for word in news:\n",
    "                try:\n",
    "                    word_index = vocab_index_dict.get(word)\n",
    "                    w = X_test_tfidf[i, word_index]\n",
    "                    word_weight.append(w)\n",
    "                except:\n",
    "                    word_weight.append(0)\n",
    "            test_weights.append(word_weight)      \n",
    "        return train_weights, test_weights\n",
    "    \n",
    "    def _text2words(self):\n",
    "        #Split each news into words\n",
    "        train_text_words = []\n",
    "        test_text_words = []\n",
    "        for text in self._train_data:\n",
    "           #Collect words for each news\n",
    "           train_text_words.append(text.split())\n",
    "        for text in self._test_data:\n",
    "            test_text_words.append(text.split())\n",
    "        return train_text_words, test_text_words\n",
    "    \n",
    "    def buildVocab(self):\n",
    "        words = []\n",
    "        for text in self._train_data:\n",
    "           #Collect all the chars\n",
    "           words.extend(text.split())\n",
    "        #Calculate frequencies of each character\n",
    "        word_freq = Counter(words)\n",
    "        #Filter out those low frequency characters\n",
    "        vocab = [u for u,v in word_freq.items() if v>3]\n",
    "        if 'UNK' not in vocab:\n",
    "            vocab.append('UNK')\n",
    "        #Map each char into an ID\n",
    "        word_id_map = dict(zip(vocab, range(len(vocab))))\n",
    "        #Map each ID into a word\n",
    "        id_word_map = dict(zip(word_id_map.values(), word_id_map.keys()))\n",
    "        return vocab, word_id_map, id_word_map\n",
    "    \n",
    "    def text2vecs(self):\n",
    "        #Map each word into an ID\n",
    "        train_text_words, test_text_words = self._text2words()\n",
    "        vocab, word_id_map, id_word_mapp = self.buildVocab()\n",
    "        def word2id(c):\n",
    "            try:\n",
    "               ID = word_id_map[c]\n",
    "            except:#Trun those less frequent words into UNK\n",
    "               ID = word_id_map['UNK']\n",
    "            return ID\n",
    "        #Turn each news into a list of word Ids\n",
    "        words_vecs = lambda words: [word2id(w) for w in words]\n",
    "        train_text_vecs = [words_vecs(words) for words in train_text_words]\n",
    "        test_text_vecs = [words_vecs(words) for words in test_text_words]\n",
    "        return train_text_vecs, test_text_vecs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 149,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "th = textHelpers(train_files, test_files)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 150,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "X_train_tfidf, X_test_tfidf, vocab_index_dict = th.tfidf_vectorizer()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Classify the texts\n",
    "\n",
    "We can use some classical and simple models initially, then we can turn to some complex models and tune parameters to optimize the performance of classification"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 151,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from sklearn.naive_bayes import MultinomialNB\n",
    "from sklearn.metrics import f1_score\n",
    "nb = MultinomialNB(0.01)\n",
    "nb.fit(X_train_tfidf, train_labels)\n",
    "preds = nb.predict(X_test_tfidf)\n",
    "micro_f1 = f1_score(test_labels, preds, average='micro')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 152,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy:0.874\n"
     ]
    }
   ],
   "source": [
    "print('Accuracy:{:.3f}'.format(micro_f1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 165,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from sklearn.ensemble import RandomForestClassifier\n",
    "rf =  RandomForestClassifier(n_estimators=100, n_jobs=4)\n",
    "rf.fit(X_train_tfidf, train_labels)\n",
    "preds = rf.predict(X_test_tfidf)\n",
    "micro_f1 = f1_score(test_labels, preds, average='micro')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 166,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.88314858130784091"
      ]
     },
     "execution_count": 166,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "micro_f1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python [default]",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
